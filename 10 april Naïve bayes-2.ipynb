{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2402b5-8747-4537-a408-efbb6e2081f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d9b516-2abd-4356-b953-7d92bb7a31b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "To calculate the probability that an employee is a smoker given that they use the health insurance plan, you can use Bayes' theorem. Here's how to do it:\n",
    "\n",
    "Let:\n",
    "- A be the event that an employee uses the health insurance plan.\n",
    "- B be the event that an employee is a smoker.\n",
    "\n",
    "You want to find \\(P(B|A)\\), which is the probability that an employee is a smoker given that they use the health insurance plan.\n",
    "\n",
    "You are given:\n",
    "\n",
    "- \\(P(A)\\), the probability that an employee uses the health insurance plan, which is 70% or 0.7.\n",
    "- \\(P(B|A)\\), the probability that an employee is a smoker given that they use the health insurance plan, which is 40% or 0.4.\n",
    "\n",
    "Now, you can use Bayes' theorem to find \\(P(B|A)\\):\n",
    "\n",
    "\\[ P(B|A) = \\frac{P(A|B) \\cdot P(B)}{P(A)} \\]\n",
    "\n",
    "We are given \\(P(B|A)\\) (0.4), and we need to find \\(P(B)\\) and \\(P(A)\\).\n",
    "\n",
    "- \\(P(B)\\) is the overall probability that an employee is a smoker, regardless of whether they use the health insurance plan. We don't have this information, so we need to calculate it using the law of total probability:\n",
    "\n",
    "\\[ P(B) = P(B|A) \\cdot P(A) + P(B|\\neg A) \\cdot P(\\neg A) \\]\n",
    "\n",
    "Where:\n",
    "- \\(P(\\neg A)\\) is the probability that an employee does not use the health insurance plan, which is \\(1 - P(A)\\).\n",
    "- \\(P(B|\\neg A)\\) is the probability that an employee is a smoker given that they do not use the health insurance plan. We don't have this information, so we'll need to make an assumption or obtain this data.\n",
    "\n",
    "Once you have \\(P(B)\\), you can substitute it into Bayes' theorem to calculate \\(P(B|A)\\):\n",
    "\n",
    "\\[ P(B|A) = \\frac{P(A|B) \\cdot P(B)}{P(A)} \\]\n",
    "\n",
    "Keep in mind that you may need additional information or assumptions about \\(P(B|\\neg A)\\) to complete this calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef4ccbb-0692-4be2-822d-2b4ead3631ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c0ecec-058c-4ed9-bda7-12ce8867546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are both variants of the Naive Bayes classifier, but they are designed for different types of data and are used in different types of classification tasks. Here are the key differences between the two:\n",
    "\n",
    "**1. Type of Data:**\n",
    "\n",
    "- **Bernoulli Naive Bayes:** This classifier is typically used when dealing with binary data or data that can be represented as binary features (i.e., features that are either present or absent). It assumes that features are binary, and it models the presence or absence of each feature.\n",
    "  \n",
    "- **Multinomial Naive Bayes:** Multinomial Naive Bayes is used when dealing with categorical data or data with discrete counts. It is well-suited for text classification tasks, where features often represent word frequencies or term counts.\n",
    "\n",
    "**2. Feature Representation:**\n",
    "\n",
    "- **Bernoulli Naive Bayes:** Features in Bernoulli Naive Bayes are binary. Each feature is considered as a binary variable, indicating whether a specific attribute or term is present (1) or absent (0).\n",
    "\n",
    "- **Multinomial Naive Bayes:** Features in Multinomial Naive Bayes are typically represented as counts of occurrences. In text classification, for example, features are often word counts within a document or the frequency of each term.\n",
    "\n",
    "**3. Probability Distribution:**\n",
    "\n",
    "- **Bernoulli Naive Bayes:** It assumes a Bernoulli distribution for the features, which is suitable for binary data. It models the probability of each feature being present or absent in each class.\n",
    "\n",
    "- **Multinomial Naive Bayes:** It assumes a Multinomial distribution for the features, which is appropriate for count-based data. It models the probability of observing different counts of each feature in each class.\n",
    "\n",
    "**4. Use Cases:**\n",
    "\n",
    "- **Bernoulli Naive Bayes:** It is commonly used in tasks where the presence or absence of specific binary features is important, such as spam detection (presence or absence of certain words or features in an email) or document classification.\n",
    "\n",
    "- **Multinomial Naive Bayes:** It is widely used in natural language processing (NLP) tasks, including text classification, sentiment analysis, and document categorization, where features represent word frequencies or term counts.\n",
    "\n",
    "**5. Smoothing:**\n",
    "\n",
    "- **Bernoulli Naive Bayes:** Laplace smoothing or additive smoothing is often applied to handle zero counts when calculating probabilities.\n",
    "\n",
    "- **Multinomial Naive Bayes:** Laplace smoothing is also commonly applied when dealing with count-based data to avoid zero probabilities.\n",
    "\n",
    "In summary, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of the data and the specific classification task. If your data consists of binary features, Bernoulli Naive Bayes is more appropriate, while Multinomial Naive Bayes is suitable for count-based or categorical data, particularly in text classification scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0bb6b0-4ca7-4fa3-a329-e6321e6a1153",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456204e4-2306-4215-8229-3ed9faa89277",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bernoulli Naive Bayes, like other Naive Bayes variants, generally assumes that features are binary, indicating whether a specific attribute or term is present (1) or absent (0). Consequently, missing values are typically treated as a form of absence. Here's how Bernoulli Naive Bayes handles missing values:\n",
    "\n",
    "1. **Missing Values as Absence (0):** In Bernoulli Naive Bayes, missing values in the feature set are usually interpreted as the absence of the corresponding feature. This means that if a feature value is missing for a particular sample, it is treated as if that feature is not present for that sample (assigned a value of 0).\n",
    "\n",
    "2. **Effect on Probability Estimation:** When estimating probabilities for each feature being present or absent in each class, the classifier takes into account the presence and absence of features based on the available data. If a feature is often missing in a particular class, the classifier will consider it as absent (0) for that class when calculating probabilities.\n",
    "\n",
    "3. **Smoothing:** In practice, Bernoulli Naive Bayes often employs smoothing techniques like Laplace smoothing (or additive smoothing) to handle missing values and avoid zero probabilities. Smoothing ensures that even if a feature is missing for some samples in a class, it doesn't result in a probability estimate of zero, which can be problematic when making predictions.\n",
    "\n",
    "4. **Impact on Classification:** Missing values can affect the classification outcome, particularly if they are not handled properly. The impact depends on the frequency and distribution of missing values in the dataset. Smoothing helps mitigate the impact to some extent.\n",
    "\n",
    "It's important to note that the handling of missing values in Bernoulli Naive Bayes can vary depending on the specific implementation or library you are using. Some implementations may offer options for customizing how missing values are treated, including the choice of smoothing techniques.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes treats missing values as the absence of corresponding features (0) and uses smoothing techniques to handle them effectively during probability estimation, ensuring that missing values do not result in problematic zero probabilities during classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff538c-5983-4a59-ac9f-097e2f6ee3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19829cc-4bb2-4926-96dd-656efd2e6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. Gaussian Naive Bayes is a variant of the Naive Bayes classifier that assumes that the features follow a Gaussian (normal) distribution within each class. While it is commonly used for binary classification tasks, it can also be extended to handle multi-class classification problems with more than two classes.\n",
    "\n",
    "In multi-class classification using Gaussian Naive Bayes, the algorithm estimates the parameters of the Gaussian distribution (mean and variance) for each feature within each class. When making predictions, it calculates the likelihood of the observed data for each class and selects the class with the highest likelihood.\n",
    "\n",
    "Here's a general overview of how Gaussian Naive Bayes can be adapted for multi-class classification:\n",
    "\n",
    "1. **Model Parameters:** For each class in the multi-class problem, Gaussian Naive Bayes estimates the mean and variance of each feature. This results in a set of parameters for each class.\n",
    "\n",
    "2. **Likelihood Calculation:** When making predictions, the classifier calculates the likelihood of the observed feature values given the estimated parameters (mean and variance) for each class. This is done using the probability density function of the Gaussian distribution.\n",
    "\n",
    "3. **Class Selection:** The class with the highest likelihood (i.e., the class that provides the best fit for the observed feature values) is selected as the predicted class.\n",
    "\n",
    "4. **Handling Class Imbalance:** In multi-class problems, it's important to consider class imbalance and adjust the prior probabilities (class priors) accordingly to avoid biasing the classifier towards the majority class.\n",
    "\n",
    "5. **Performance Evaluation:** Common performance metrics like accuracy, precision, recall, and F1-score can be used to evaluate the performance of the Gaussian Naive Bayes classifier in multi-class classification tasks.\n",
    "\n",
    "While Gaussian Naive Bayes can be used for multi-class classification, it's worth noting that it makes the assumption of Gaussian distribution for the features within each class. This assumption may not always hold for all types of data, and the classifier's performance can be influenced by the appropriateness of this assumption. In cases where the Gaussian assumption is not met, other classifiers, such as Multinomial Naive Bayes or decision trees, may be more suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b8387-51dd-4d49-824a-5cb234a7b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a24fb76-c9a2-4092-b061-9db6652f94df",
   "metadata": {},
   "outputs": [],
   "source": [
    "I'll provide you with a step-by-step guide on how to perform the described tasks using Python and scikit-learn. Here's how you can implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers, evaluate their performance using 10-fold cross-validation, and report the performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245f15d8-98e6-425b-8563-0287a6a3bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Implementation Steps\n",
    "Data Preparation:\n",
    "\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository.\n",
    "Load the dataset into a pandas DataFrame or using other suitable methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac9b3b2-4f53-46fb-803e-b96df4ee570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae6f9cfd-2834-4d4c-a5dc-80d09732d16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59ac5fc-c583-47b9-bc8c-d77d4a74852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Splitting:\n",
    "\n",
    "Split the dataset into features (X) and the target variable (y), where y represents whether an email is spam or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2985601e-0458-4dee-bc41-a9c904d8f02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifier Implementation and Cross-Validation:\n",
    "\n",
    "Initialize the three Naive Bayes classifiers: BernoulliNB, MultinomialNB, and GaussianNB.\n",
    "Use 10-fold cross-validation to evaluate each classifier's performance. Calculate accuracy, precision, recall, and F1 score for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd1e338-88ad-48f3-845b-ff5bad6c0d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Perform cross-validation and calculate metrics\n",
    "def evaluate_classifier(classifier, X, y):\n",
    "    accuracy_scores = cross_val_score(classifier, X, y, cv=10, scoring='accuracy')\n",
    "    precision_scores = cross_val_score(classifier, X, y, cv=10, scoring='precision')\n",
    "    recall_scores = cross_val_score(classifier, X, y, cv=10, scoring='recall')\n",
    "    f1_scores = cross_val_score(classifier, X, y, cv=10, scoring='f1')\n",
    "    return {\n",
    "        'Accuracy': accuracy_scores.mean(),\n",
    "        'Precision': precision_scores.mean(),\n",
    "        'Recall': recall_scores.mean(),\n",
    "        'F1 Score': f1_scores.mean()\n",
    "    }\n",
    "\n",
    "# Evaluate Bernoulli Naive Bayes\n",
    "bernoulli_metrics = evaluate_classifier(bernoulli_nb, X, y)\n",
    "\n",
    "# Evaluate Multinomial Naive Bayes\n",
    "multinomial_metrics = evaluate_classifier(multinomial_nb, X, y)\n",
    "\n",
    "# Evaluate Gaussian Naive Bayes\n",
    "gaussian_metrics = evaluate_classifier(gaussian_nb, X, y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
